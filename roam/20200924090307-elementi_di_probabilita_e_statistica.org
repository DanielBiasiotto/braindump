#+TITLE: Elementi di Probabilita' e Statistica
#+COURSE: EPS A
#+PROF:Roberta Sirovich
#+STARTUP: latexpreview
[[file:#universita.org][#universitá]]

+ r-project.org
+ Editor: r-studio
+ Esame:
  - Quiz Moodle
    - Esercizi Probabilita'
    - Esercizi Statistica (r)
+ Libri:
  - Bertsekasi - Tsitsiklis ~ Introduction to Probability
    * ottimo testo di teoria (MIT)
  - Ross ~ Introduction to Probability and Statistics for Engineers and Scientists
    * fonte di esercizi
  - Verzani ~ Using R for Introductory Statistics
    * esercizi di statistica
+ Esercizi aggiuntivi Pearson
  - secondo Moodle


* Probabilita'
** descrive eventi soggetti a ~casualita'~
- nasce nell'ambito del gioco d'azzardo ~ seconda meta' del 800

*** esperimenti il cui esito non e' prevedibile a priori

*** casualita' legata alla complessita'

- sarebbe descrivibile con strumenti classici ma cio' solo con sistemi _troppo_ complessi, che non sappiamo risolvere

** Insieme
*** collezione di oggetti (elementi)

*** Naturali - contiene un numero n di elementi (finito o infinito)

**** sono tutti elencabili

***** infinito numerabile

*** Reali

**** infinito non numerabile | piu' che numerabile

***** descritti con una proprieta'

*** Uso in Probabilita'
gli insiemi sono usati per descrivere gli esiti possibili degli esperimenti probabilistici


*** Se tutti gli elementi di A sono membri di B

**** A e' contenuto in B

*** Omega

**** insieme di ~tutti~ i possibili esiti

*** Partizioni Ai di S

**** insiemi che coprono S

**** insiemi disgiunti a due a due

*** Leggi di de Morgan

** Probabilitá di Eventi

*** Modello probabilistico
**** descrizione matematica del
- esperimento probabilistico che stiamo analizzando
  - spazio campionario
  - ~legge di probabilita'~
    - ad ogni esito (sottoinsieme di Omega) assegna un numero positivo detto ~probabilita'~ di tale evento

**** =def= Legge di Probabilita' P
Insieme d'arrivo [0,1]
**** Assiomi

***** Positivita'

***** Misura Finita
Probabilita' dello spazio campionario vale 1

***** Additivita'

**** Proprieta' della P dedotte dagli assiomi

***** Monotonia

***** P di Unione di 2 ins

***** P di unione <= P degli insiemi

***** P di Unione di 3 ins

**** Modelli Continui
- Probabilita' come Area

**** Esempi di P - Legge Uniforme Discreta

- Frequenza Relativa di occorrenza
- P come area
- moneta equa
  - non truccata: le due facce sono =equipossibili=
- moneta equa lanciata 3 volte
  - escano due teste
- due volte dado a 4 facce equo

**** Probabilita' Condizionata
Misura che permette di calcolare probabilita' in condizioni di informazione parziale
- Prende in considerazione informazioni
\(P(A|B)=\frac{P(A\cap B)}{P(B)}\) o equivalentemente \(P(A\cap B) = P(A|B)\cdot P(B)\)
\(P(A_1\cup A_2 | B) = P(A_1|B)+P(A_2|B)-P(A_1\cap A_2 |B)\)

***** regola della moltiplicazione
Per eventi indipendenti
\begin{equation*}
\begin{split}
P(A_1\cap A_2 \cap ... \cap A_n) = & P(A_n | A_1 \cap A_2 \cap ... \cap A_{n-1}) \cdot \\
                               & P(A_{n-1}|A_1 \cap A_2 \cap ... \cap A_{n-2}) \cdot \\
                               & ... \\
                               & P(A_2|A_1) \cdot P(A_1)
\end{split}
\end{equation*}
***** Formula delle probabilita' totali
In caso di $n$ partizioni di \(\Omega\) dette \(A_i\)
\begin{equation*}
\begin{align}
P(B) = & P(B|A_1)\cdot P(A_1) + \\
       & P(B|A_2)\cdot P(A_2) + \\
       & ... \\
       & P(B|A_n)\cdot P(A_n)
\end{align}
\end{equation*}

***** Formula di Bayes
In caso di $n$ partizioni di \(\Omega\) dette \(A_i\)
$P(A_i|B)=\frac{P(B|A_i)P(A_i)}{P(B)}$

**** Eventi indipendenti
    ~NB~ l'indipendenza é una proprietá delle probabilitá, non degli eventi: dipende dalle misure
    Due eventi sono indipendenti se
        \(P(A|B)=P(A)\)

    Allora:
        \(\frac{P(A\cap B)}{P(B)} = P(A)\)
        \(P(A\cap B) = P(A)P(B)\)

    =def equivalente=
    Due eventi sono indipendenti se
        \(P(A\cap B) = P(A)P(B)\)

        \(P(B|A)P(A) / P(B)= P(A)\) per Bayes
        \(P(B|A)=P(B)\)

    =def equivalente=
        \(P(B|A)=P(B)\)

    - concetto intuitivo di indipendenza
      - due dadi lanciati sono indipendenti dal punto di vista meccanicistico

    Eventi senza intersezione non nulli non sono mai indipendenti
    - Quindi A e A complementare sono sempre dipendenti
      - quindi dati A e B indipendenti questi sono indipendenti dai complementari altrui

***** Indipendenza a due a due
\begin{equation*}
\begin{align}
P(\{(i,j)\}) & = P(A_i \cap B_j) \\
             & = P(A_i) \cdot P(B_j) \\
             & = P^1(\{i\}) \cdot P^2(\{j\})
\end{align}
\end{equation*}
***** Indipendenza Condizionata
$A$ e $B$ sono indipendenti condizionatamente a $C$ se:
\begin{equation*}
\begin{align}
P(A \cap B | C) = P(A|C) \cdot P(B|C)
\end{align}
\end{equation*}
***** Moltiplicazione Cartesiana
    La \(P\) di duple o n-uple é
    \(P=P^1*P^2*...*P^n\)

** Variabili Aleatorie
    In casi in cui l'esperimento é numerico servono altri strumenti rispetto a quelli usati fino ad ora
    Una Variabile Aleatoria é una funzione da \(\Omega\) in \(\mathbb{R}\)
    - \(X,Y,Z\) variabili aleatorie
    - \(x,y,z\) punti di \(\mathbb{R}\)
    Dipendentemente dall'immagine della variabile aleatoria questa sará discreta o continua

*** Funzione della massa di Probabilitá =PMF=
    aka =Probability Mass Function=
    Probabilitá che una variabili aleatoria $X$ valga esattamente $x$, valore arbitrario
    $p_X(x_i)=P(X = x_i)$
    - il grafico della funzione mostra la natura discreta o il continua della variabile $X$
    Dato che le variabili aleatorie sono funzioni:
    Le intersezioni delle controimmagini $X_{-1}$ sono a due a due disgiunte e comprono tutto $\Omega$
    $\Rightarrow \sum_{x\in Im(X)} p_X(x)=1$

*** Funzioni di Variabili Aleatorie
dove \(Y=g\comp X\)
\(P_Y(y)= \sum_{x\in g^-1(\{y\})}P_X(x)\)
*** Variabili Aleatorie Discrete
**** Bernulliana
Singola prova con risultato dicotomico

**** Binomiale
$p_X(k)=\binom{n}{k}p^k(1-p)^{n-k}$ con $k \in Im(X)=\{1,2,...\}$
**** Geometrica
$p_X(k)=(1-p)^{1-p}p$ con $k\in Im(X)=\{1,2,...\}$
**** Poisson
\(X \sim \text{Poisson}(\lambda)\) con $\lambda$ intensitá
\(p_X(k) = \frac{\lambda^k e^{-\lambda}}{k!}\)
**** Ipergeometrica
\(p_X(k) = \frac{\binom{C}{k}\binom{N-C}{n-k}}{\binom{N}{n}}\)

**** Media
Informazioni riassuntive, piú semplici anche se parziali
\(E(X) = \sum_{x\in Im(X)}x p_X(x)\)
- media pesata sulle $p_X$ delle singole $x$
Questa ha il significato di $\lambda$ nella Poisson

**** Momento
di ordine $k$
\(m_k = E(X^k) = \sum_{x \in Im(X)} x^2  p_X(x)\)

**** Varianza
Anche detto scarto quadratico medio
\(Var(X) = E([X-E(X)]^2)\)
\(Var(aX+b) = a^2 Var(X)\)
- quadratica nelle costante moltiplicative
- invariante per traslazioni

**** Deviazione Standard
Radice della Varianza
*** Variabili Aleatorie Composte
=Indipendenza=
\( p_{X,Y}(x,y) = p_X(x)p_Y(y) \) per generici $x$ e $y$

*** Variabili Aleatorie Continue
[[ﬁle:/home/dan/Documents/UNI/II/EPS A/epsA-04dic20.pdf][Appunti Prof]]
\(Im(X)\) di natura continua
- non possiamo usare la $PMF$
  + in quanto l'immagine non é numerabile, non possiamo associare ad ogni punto un valore
Dato l'intervallo $A$ abbiamo quindi una funzione \(f_x: \mathbb{R} \to \mathbb{R} \) non negativa t.c
\(\forall A \subset \mathbb{R}\):
- \(\mathbb{P}(X \in A) = \int_A f_{X}(t)dt\)
  + includendo o meno gli estremi la $P$ non cambia
    - \(\mathbb{P}(X \in A) = \mathbb{P}(X = a) = \int_a^a f_X(t)dt = 0\)
Questa é detta $PDF$ =Probability Density Function=

**** Proprietá
- i\(\int_\mathbb{R} f_X(t)dt = P(\Omega) = 1\)
- \(\forall x \in \mathbb{R}, f_X(x) \ge 0\)
- \(f_X(x)\) non é una probabilitá
  + puó essere un valore qualunque, anche maggiore di 1
- \(\int_x^{x+\delta}f_X(t)dt = f_X(x)\cdot\delta\)
  + se $\delta$ sufficientemente piccolo

**** Uniforme
$X$ ha densitá costante nell'intervallo
$f_X(x) = \frac{1}{b-a}$ nell'intervallo
       $=0$   altrove

**** Esponenziale
$\mathbb{P}(X\in A), A \subset (-\inf, 0)$ é nulla
La sua Densitá decade
\( f_X(x) = \lambda\cdot e^{-\lambda x} \) se \(x \ge 0\)
      \( = 0\) altrove
- $X$ prende solo valori positivi
Vale Proprietá di Assenza della Memoria
- \( \mathbb{P}(X > m \mid X>n) = \mathbb{P}(X> m-n) \)
  + cambia l'origine dei tempi nel calcolo della probabilitá di sopravvivenza
- =NB= $PDF$ simile alla $PMF$ della Geometrica
  + anche in quella Variabile Aleatoria vale la Proprietá di Assenza della Memoria
* Statistica
