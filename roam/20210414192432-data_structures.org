#+title: Data Structures
* Abstract Data Type
Tipi predefiniti sono forniti dai linguaggi
- ogni tipo é associato con un insieme di valori e operatori

Una implementazione concreta di un ADT é
- struttura dati
- collezione di procedure con cui realizzare le operazioni

La relazione fra tipo astratto e struttura concreta é analoga a quella fra problema algoritmico e algoritmo

* Strutture Dati
** Insiemi Dinamici
Possibili in array, liste o hash, ma con tempo di calcolo diversi
- elementi finiti
- gli elementi possono cambiare
- il loro numero puó cambiare
- si assume che ogni elemento abbia un attributo chiave
- si assume che le chiavi sono tutte distinte

2 tipi di operazione
- query
- modifiche
tipicamente
- insert
- search
- delete
inoltre hanno senso se l'insieme é ordinato (le chiavi sono ordinate)
- minimum
- maximum
- successor
- predecessor

Strutture dati diverse hanno diversa complessitá con operazioni diverse
- ogni struttura ha efficienza migliore in base a ció che si va a sviluppare

** Array
caselle
- contengono un elemento
- sono grandi uguali
- posizionati in una sequenza nella memoria
- il ~calcolo dell'indirizzo~ di qualunque casella ho costo ~costante~
  + non dipende dal numero di elementi
  + base + (i-1) * dim(valore)
- =accesso diretto=
  + $O(1)$

*** Statici
Numero massimo di elementi prefissato
- M: numero massimo
  - non ha impatti sui tempi di calcolo
- N: numero attuale de elementi
  + occupano sempre le prime N celle dell'array
**** Insert
Senza controllo sulla ripetizione di chiave
#+begin-example
ArrayInsert(A,k):
  if A.N != A.M
    A[A.N] = k
    A.N++
    return k
  else
    return nil
#+end-example
$O(1)$ costante

Se l'Array é ordinato gli inserimenti costano di piú
- inserendo k in fondo
- far scendere alla posizione giusta con scambi (=InsertionSort=)
$O(N)$

**** Delete
Assumendo non ci siano ripetizioni
#+begin-example
ArrayDelete(A, k):
  for i=1 to A.N do
    if A[i] == k then
      A.N = A.N + 1
      for j=i to A.N do
        A[j] = A[j+1]
      return k
  return nil
#+end-example
$O(N)$ lineare
**** Search
#+begin-example
ArraySearch(A,k):
  for i=1 to A.N do
    if A[i] == k then
      return k
  return nil
#+end-example
$O(N)$ lineare

Se l'Array é ordinato allora possiamo implementare la =BinarySearch=
$O(n \log n)$

*** Ridimensionabili / Dinamici
Se non si conosce il numero massimo di elementi a priori
Se non si vuole sprecare spazio

Le 2 idee utilizzate sono due soluzioni diverse per la realizzazione di un =Abstract Data Type=
- per confrontarle valutiamo i tempi di esecuzione di =sequenze di operazioni=
  + consideriamo allora il costo medio per i confronti: =costo ammortizzato=
    - costo ammortizzato nel futuro anche se costoso subito

- $2^k$ inserimenti, con M=1 inizialmente
  1. ogni inserimento tranne il primo ha costo $O(N)$
     - $T_{amm} = \frac{d + c + 2c + ... + (n-1)c}{n} \in O(n)$
       - sopra abbiamo una progressione aritmetica
         + numeratore di secondo grado, denominatore di primo grado
  2. $k$ inserimenti con $O(N)$, gli altri $O(1)$
     - $T_{amm}= \frac{(c+2c+...+2^{k-1}c)+2^kd}{2^k}$
     - $T_{amm}= \frac{(2^k -1)c+2^kd}{2^k} \in O(1)$
- sequenza di rimozioni di elementi
- sequenza di inserimenti, ma aumentando la dimensione dell'array di una costante se riempito

**** Extend
Si basa sull'espandere l'array quando esso diventa troppo piccolo
- l'espansione costa $O(N)$ in quanto richiede di allocare memoria e copiare gli elementi dell'array
#+begin-example
ArrayExtend(A,n):
  B = array[A.M + n]
  B.M = A.M + n
  B.N = A.N
  for i=1 to A.N do
    B[i] = A[i]
  return B
#+end-example

Il problema é che se N == M allora i successivi inserimenti richiedono ulteriori riallocazioni

**** Insert
#+begin-example
DynArrayInsert(A,k):
  if A.N == A.M then
    A = ArrayExtend(A,1)
  ArrayInsert(A,k)
#+end-example
Array non pieno: $O(1)$
Array pieno: $O(N)$
Dipende dalle operazioni precendenti
- se M é sufficientemente grande si sforeranno poche volte
  + il costo sará circa $O(1)$ ma si rischia di sprecare spazio
- se M é tale da sforare molte volte
  + il costo sará circa $O(N)$

Il problema é che se N == M allora i successivi inserimenti richiedono ulteriori riallocazioni
- raddoppiamo il numero di elementi se l'array si riempie
#+begin-example
DynArrayInsert(A,k):
  if A.N == A.M then
    A = ArrayExtend(A,A.M)
  ArrayInsert(A,k)
#+end-example

**** Delete
Possiamo recuperare spazio se l'array si riduce di dimensione
#+begin-example
DynArrayDelete2(A,k):
  ArrayDelete(A,k)
  if A.N <= 1/4 * A.M then
    B = array[A.M/2]
    B.M = A.M/2
    B.N = A.N
    for i=1 to A.N do
      B[i] = A[i]
    A = B
#+end-example

**** Search
** Liste
Una struttura lineare
- l'ordine é determinato dai puntatori che indicano l'elemento successivo
- data una lista L il primo elemento é indicano da L.head
Puó essere doppiamente concatenata
- con puntatori .prev e .next
Puó essere
- ordinata
- non ordinata
Puó essere circolare
- l'ultimo elemento punta il primo e viceversa
  + permette di accedere all'ultimo elemento piú facilmente

*** Insert
In Liste doppiamente concatenate e non ordinate:
#+begin-example
ListInsert(L,x):
  x.next = L.head
  x.prev = nil
  if L.head != nil then
    L.head.prev = x
  L.head = x
#+end-example
$O(1)$

Con sentinella:
#+begin-example
ListInsert(L,x):
  x.next = L.sen.next
  L.sen.next.prev = x
  L.sen.next = x
  x.prev = L.sen
#+end-example
$O(1)$

*** Delete
In Liste doppiamente concatenate e non ordinate:
- ricevendo un puntatore al nodo da rimuovere
#+begin-example
ListDelete(L,x):
  if x.prev != nil then
    x.prev.next = x.next
  else
    L.head = x.next
  if x.next != nil then
    x.next.prev = x.prev
#+end-example
$O(1)$

L'operazione é macchinosa perché bisogna controllare le condizioni "in testa" e "in coda"
- aggiungiamo nodo ~sentinella~ che rende piú omogenei i dati nella lista
  + Lista circolare
Si ha sempre la certezza che la lista contenga sempre almeno un elemento:
#+begin-example
ListDelete(L,x):
  x.prev.next = x.next
  x.next.prev = x.prev
#+end-example
$O(1)$ comunque tempo costante minore che senza sentinella
Ma il codice diventa piú semplice e leggibile

*** Search
In Liste doppiamente concatenate e non ordinate:
#+begin-example
ListSearch(L,x):
  x = L.head
  while x != nil and x.key != k do
    x = x.next
  return x
#+end-example
$O(N)$

Con sentinella:
#+begin-example
ListSearch(L,x):
  x = L.sen.next
  while x != L.sen and x.key != k do
    x = x.next
  return x
#+end-example
$O(N)$

** Hashing
*** Tavole a indirizzamente diretto
$U$ universo delle chiavi, tutte le chiavi possibili
- interi positivi tra 0 e m-1
L'insieme viene rappresentato con un array $T$ con dimensione $m$
- quindi si ha indirizzamento diretto

L'insieme delle chiavi $S$ é sottoinsieme di $U$
- ogni posizione di T contiene un puntatore hai dati con la chiave associata
#+begin-example
TableInsert(T,x):
  T[x.key] = x
TableDelete(T,x):
  T[x.key] = nil
TableSearch(T,k):
  return T[k]
#+end-example
Tutte le operazioni hanno complessita temporale $O(1)$

Sembra molto efficiente temporalmente
Non lo é sempre in termini di spazio
- se l'universo é molto grande occuperó molto spazio dovendo avere un puntatore per ognuna delle chiavi dell'universo

*** Tavole di hash
Possiamo utilizzare una tabella $T$ di dimensione $m$ molto piú piccola di $|U|$
- allora la posizione della chiave $k$ é determinata utilizzando una funzione di hash in quanto non c'é piú corrispondenza diretta tra chiave e indice
Quindi:
- l'indirizzamento non é piú diretto
- la posizione di $k$ é $h(k)$
- 2 chiavi possono corrispondere alla stessa posizione di $T$
  + una buona funzione hash riduce al minimo il numero di collisioni, cercando di distribuire in maniera casuale le coppie $k$ e posizione

*hash perfetto*
- funzione che non crea mai collisione, cioé una ~funzione iniettiva~:
  - $k_1 \neq k_2 \implies h(k_1) \neq h(k_2)$

*** Tavole hash con concatenamento
Cercano di risolvere le collisioni:
- usiamo liste per gestirle
  - allora se c'é collisione l'elemento é inserito in testa alla lista puntata dal puntatore nella posizione della array T

Il calcolo del hash ha tempo costante $O(1)$

#+begin-example
HashInsert(T,x):
  L = T[h(x.key)]
  ListInsert(L,x)
#+end-example
$O(1)$

#+begin-example
HashDelete(T,x):
  L = T[h(x.key)]
  ListDelete(L,x)
#+end-example
$O(1)$ perché la lista é doppiamente concatenata
- di un elemento giá individuato

#+begin-example
HashSearch(T,k):
  L = T[h(k)]
  return ListSearch(L,k)
#+end-example
La ricerca di un elemento richiede un tempo proporzionale alla lunghezza hella lista T[h(k)]
Definiamo:
- $m$ numero di celle in $T$
- $N$ numero di elementi memorizzati
- $\alpha = N / m$ fattore di carico
  - lunghezza media delle liste contenute nella tabella hash
~caso peggiore~:
- inserimenti con la stessa hash
- tutte le chiavi sono associate con la stessa cella di T
- ricerca $O(N)$
~caso migliore~:
- quando la lista T[h(k)] é vuota o contiene un solo elemento
- ricerca $O(1)$
~caso medio~:
- dipende dalla funzione hash
  - assumiamo $O(1)$
  - gode della proprietá di uniformitá semplice
    + =uniformitá semplice=
      + distribuisce in modo uniforme le chiavi fra le celle
        - ogni cella ha la stessa probabilitá di essere destinazione di una chiave casuale
- ricerca di un elemento non presente
  + individuare la lista é $\Theta(1)$
  + ogni lista ha la stessa probabilitá di essere associata con la chiave
  + percorrere la lista costa in media $\Theta(\alpha)$
  + infine il tempo richiesto é $\Theta(1+\alpha)$
    - $\alpha$ non é costante!
- ricerca di un elemento che c'é
  - dipende da dove l'elemento si colloca nella lista
  - il tempo di individuare la lista é $\Theta(1)$
  - assumiamo che la ricerca riguardi l'i-esimo elemento inserito
    + quanti di questi finiscono nella lista di $x_i$?
    + ogni elemento viene inserito nella lista di $x_i$ con probabilitá $\frac{1}{m}$
    + in media $\frac{N-i}{m}$ elementi precedono $x_i$ nella lista di $x_i$
  - il tempo di ricerca di $x_i$ é proporzionale a $1+\frac{N-i}{m}$
  - generalizzando alla ricerca di un elemento a caso
    + $\frac{1}{N} \sum_{i=1}^{N}(1+\frac{N-i}{m})$
    + $1+ \frac{\alpha}{2} - \frac{\alpha}{2N}$
  - in totale:
    + $\Theta(1) + \Theta(1+ \frac{\alpha}{2} - \frac{\alpha}{2N}) = \Theta(1+\alpha)$
Si conclude che se il numero di celle in T é proporzionale a N allora $N = O(m)$ e quindi $\alpha = O(1)$ e la ricerca richiede tempo $O(1)$

*** Funzioni hash
Solitamente la distribuzione secondo la quale si estagono le chiavi nno é nota
- non si puó creare una funzione hash perfetta
In un Computer le chiavi sono interpretate come sequenze di bit
- si cerca di utilizzare ogni bit della chiave
- una buona funzione hash sceglie posizioni in modo tale da eliminare eventuale regalaritá nei dati

**** Metodo della divisione
Veloce ma dipende da m
- m potenza di 2 é una buona scelta solo se si ha la certezza che gli ultimi bit hanno distribuzione uniforme

****   Metodo della moltiplicazione
m non é critico, solitamente si sceglie una potenza di 2
A dipende dai dati
- una scelta rogionevole empiricamente é $(\sqrt{5} - 1) / 2$
*** Indirizzamente aperto
Tutti gli elementi sono memorizzati nella tavola T
- l'elemento con chiave k viene inserito nella posizione h(k) se é libera
- se non é libera allora si cerca una posizione libera secondo uno ~schema di ispezione~
Possono avvenire collisioni anche tra elementi con chiavi diverse
In generale si definisce una funzione hash estese con l'ordene di ispezione
- $h: U \times \{0,1,2,...,m-1\} \to \{0,1,2,...,m-1\}$

=ispezione lineare=
- crea file di celle occupate, ovvero ~addensamento primario~
=ispezione quadratica=
- $h(k,i) = (h^{'}(k) + c_1i + c_2i^2) \mod m$
  - l'ordine con cui vengono esaminate le celle dipende solo dal hash della chiave k, ~addensamento secondario~
Per risolvere questo addensamento si introduce il =doppio hashing=
- $h(k,i) = (h_1(k) + ih_2(k)) \mod m$
  - permette una uguale probabilitá per ogni sequenza di ispezione


**** Insert
#+begin-example
HashInsert(T,x):
  for i=0 to i < m do
    j = h(x.key,i)
    if T[j] == nil then
      T[j] = x
      return j
  return nil
#+end-example

**** Search
#+begin-example
HashSearch(T,k):
  for i=0 to i < m do
    j = h(x.key,i)
    if T[j] == nil then
      return nil
    if T[j].key == k then
      return T[j]
  return nil
#+end-example

~caso ottimale~
- posizione di una chiave scelta a caso ha distribuzione uniforme
- qualunque sequenza di ispezione ha la stessa probabilitá
- Elemento Assente
  + $X$ numero di celle esaminate durante una ricerca senza successo
  + $X$ é una variabile aleatoria
    - almeno 1:
      - $P(X\ge 1)=1$
    - se la prima cella é occupata dovremo esaminare 2 celle:
      - $P(X\ge 2)= \frac{N}{m}$
    - se la seconda cella é occupata dovremo esaminare 3 celle:
      - $P(X\ge 3)= \frac{N}{m}\frac{N-1}{m-1}$
    - $P(X\ge i) \le \alpha^{i-1}}$
    - $E[X] = \sum_{i=1}^{\infty}(X \ge i) \le \sum_{i=1}^{\infty} \alpha^{i-1}=\frac{1}{1-\alpha}$
- Elemento Presente
  - sicuramente meno celle da esaminare che nel caso dell'elemento assente

**** Delete
Inserire nil creerebbe buchi nella tabella
- si potrebbe marcare le con costanti =deleted=
Di solito l'indirizzamento aperto si usa quando non c'é necessitá di cancellazione di elementi

** Pile
Inserisce chiavi in cima, rimuove le chiavi dalla cima
Come ADT:
- collezione di dati
  + elementi qualunque di tipo T
- operazioni
  + void Pusn(Stack S, T t)
  + T Pop(Stack S)
  + T Top(Stack S)
  + bool Empty(Stack S)
  + int Size(Stack S)

*** applicazioni
- chiamate ricorsive di funzioni
- visita in profonditá di grafi
- valutazione di un'espressione un notazione postfissa


*** assiomi
- Size(S), Empty(S), Push(S,t)
  + smepre definiti
- Pop(S), Top(S)
  + sono definite iff Empty(S) é false
- Empty(S)
  + true iff Size(S) é 0
- Push(S,t); Pop(S)
  + restituisce t e non modifica S
- Push(S,t); Top(S)
  + restituisce t
- Push(S,t)
  + incremento Size(S) di uno
- Pop(S)
  + decrementa Size(S) di uno

*** implementazione con array
statico di M celle
- assioma ulteriore
  + Push(S,t)
    - iff Size(S) < M
meccanismo =LIFO=

Complessitá
- Temporale $O(1)$
- Spaziale $O(M)$


#+begin_example
Push(S,t)
  if S.N != S.M then
    S.N = S.N + 1
    S[N] = t
  else
    error overflow
#+end_example

#+begin_example
Size(S)
  return S.N
#+end_example

#+begin_example
Empty(S)
  if S.N == 0 then
    return true
  else
    return false
#+end_example

#+begin_example
Top(S)
  if S.N == 0 then
    error underflow
  else
    return S[S.N]
#+end_example

#+begin_example
Pop(S)
  if S.N == 0 then
    error underflow
  else
    S.N = S.N - 1
    return S[S.N+1]
#+end_example

*** implementazione con lista
Conviene una lista semplice con sentinella
Complessitá
- Temporale $O(1)$
- Spaziale $O(N)$
  + ma con overhead dovuto ai puntatori
  + non abbiamo un limite al massimo degli elementi
#+begin_example
Push(S,t)
  S.N = S.N + 1
  t.next = S.sen.next
  S.sen.next = t
#+end_example

#+begin_example
Size(S)
  return S.N
#+end_example
#+begin_example
Empty(S)
  if S.N == 0 then
    return true
  else
    return false
#+end_example
#+begin_example
Top(S)
  if S.N == 0 then
    error underflow
  else
    return S.sen.next
#+end_example

#+begin_example
Pop(S)
  if S.N == 0 then
    error underflow
  else
    S.N = S.N - 1
    t = S.sen.next
    S.sen.next = S.sen.next.next
    return t
#+end_example


** Code
- collezione di dati
  + elementi qualunque di tipo T
- operazioni
  + void Enqueue(Queue Q, T t)
  + T Dequeue (Queue Q)
  + T Front(Queue Q)

- Utilizzi
  + buffer
  + visita in ampiezza di grafi
  + simulazione di eventi complessi

*** assiomi
- Size Empty Enqueue
  + sempre definiti
- dequeue front
  + definiti iff Empty é false
- empty size e front
  + non modificano la coda
- empty
  + true iff Size é 0
- size; enqueue(Q,t)
  + N, dopo N esecuzioni di Dequeue(Q) abbiamo Front(Q) = t
- Front(Q) = t ==> Dequeue(Q) = t
- Enqueue(Q,t) incrementa Size(Q) di 1
- Dequeue(Q) decrementa Size(Q) di 1

*** implementazione con array
statico
si temgono elementi nelle ultime posizioni
- altrimenti sarebbe necessario spostare tutti gli elementi
si usa l'array un maniera circolare
- si utilizzano riferimenti tail e head per tener conto delle posizioni dell'array
  + Q.head indica dove estrarre
  + Q.tail indica dove inserire
    - se head == tail allora la coda é vuota

Complessitá
- Temporale $O(1)$
- Spaziale $O(M)$
#+begin_example
Size(Q)
  if Q.tail >= Q.head then
    return Q.tail - Q.head
  else
    return Q.M - (Q.head - Q.tail)
#+end_example

#+begin_example
Empty(Q)
  if Q.tail == Q.head then
    return true
  else
    return false
#+end_example
#+begin_example
NextCell(Q,c)
  if c != Q.M then
    return c+1
  else
    return 1
#+end_example
#+begin_example
Enqueue(Q,t)
  if Q.tail == Q.head-1 then
    error overflow
  else
    Q[Q.tail] = t
    Q.tail = NextCell(Q,Q.tail)
#+end_example
#+begin_example
Front(Q)
  if Q.tail == Q.head then
    error underflow
  else
    return Q[Q.head]
#+end_example
#+begin_example
Dequeue(Q)
  if Size(Q) == 0 then
    error underflow
  else
    t = Q[Q.head]
    Q.head = NextCell(Q,Q.head)
    return t
#+end_example
*** implementazione con lista
inserimenti in testa, estrazioni in coda
- lista semplice con puntatore all'ultimo elemento della coda

Q.head punta l'elemento da estrarre
Q.tail punta l'ultimo elemento inserito
Q.head == nil <==> la coda é vuota
Q.N tiene conto del numero di elementi


Complessitá
- Temporale $O(1)$
- Spaziale $O(N)$
  - con overhead dei puntatori
  - non abbiamo il vincolo del massimo degli elementi

#+begin_example
Enqueue(Q,t)
  if Q.N == 0 then
    Q.head = t
    Q.tail = t
  else
    Q.tail.next = t
    Q.tail = t
  Q.N = Q.N +1
#+end_example
#+begin_example
Size(Q)
  return Q.N
#+end_example
#+begin_example
Empty(Q)
  return Q.N == 0
#+end_example
#+begin_example
Front(Q)
  if Empty(Q) then
    error underflow
  else
   return Q.head
#+end_example

#+begin_example
Dequeue(Q)
  if Empty(Q) then
    error underflow
  else
    t = Q.head
    Q.head = Q.head.next
    Q.N = Q.N -1
    return t
#+end_example
