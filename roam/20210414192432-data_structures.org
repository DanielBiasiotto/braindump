#+title: Data Structures
* Abstract Data Type
Tipi predefiniti sono forniti dai linguaggi
- ogni tipo é associato con un insieme di valori e operatori

Una implementazione concreta di un ADT é
- struttura dati
- collezione di procedure con cui realizzare le operazioni

La relazione fra tipo astratto e struttura concreta é analoga a quella fra problema algoritmico e algoritmo

* Strutture Dati
** Insiemi Dinamici
Possibili in array, liste o hash, ma con tempo di calcolo diversi
- elementi finiti
- gli elementi possono cambiare
- il loro numero puó cambiare
- si assume che ogni elemento abbia un attributo chiave
- si assume che le chiavi sono tutte distinte

2 tipi di operazione
- query
- modifiche
tipicamente
- insert
- search
- delete
inoltre hanno senso se l'insieme é ordinato (le chiavi sono ordinate)
- minimum
- maximum
- successor
- predecessor

Strutture dati diverse hanno diversa complessitá con operazioni diverse
- ogni struttura ha efficienza migliore in base a ció che si va a sviluppare

** Array
caselle
- contengono un elemento
- sono grandi uguali
- posizionati in una sequenza nella memoria
- il ~calcolo dell'indirizzo~ di qualunque casella ho costo ~costante~
  + non dipende dal numero di elementi
  + base + (i-1) * dim(valore)
- =accesso diretto=
  + $O(1)$

*** Statici
Numero massimo di elementi prefissato
- M: numero massimo
  - non ha impatti sui tempi di calcolo
- N: numero attuale de elementi
  + occupano sempre le prime N celle dell'array
**** Insert
Senza controllo sulla ripetizione di chiave
#+begin-example
ArrayInsert(A,k):
  if A.N != A.M
    A[A.N] = k
    A.N++
    return k
  else
    return nil
#+end-example
$O(1)$ costante

Se l'Array é ordinato gli inserimenti costano di piú
- inserendo k in fondo
- far scendere alla posizione giusta con scambi (=InsertionSort=)
$O(N)$

**** Delete
Assumendo non ci siano ripetizioni
#+begin-example
ArrayDelete(A, k):
  for i=1 to A.N do
    if A[i] == k then
      A.N = A.N + 1
      for j=i to A.N do
        A[j] = A[j+1]
      return k
  return nil
#+end-example
$O(N)$ lineare
**** Search
#+begin-example
ArraySearch(A,k):
  for i=1 to A.N do
    if A[i] == k then
      return k
  return nil
#+end-example
$O(N)$ lineare

Se l'Array é ordinato allora possiamo implementare la =BinarySearch=
$O(n \log n)$

*** Ridimensionabili / Dinamici
Se non si conosce il numero massimo di elementi a priori
Se non si vuole sprecare spazio

Le 2 idee utilizzate sono due soluzioni diverse per la realizzazione di un =Abstract Data Type=
- per confrontarle valutiamo i tempi di esecuzione di =sequenze di operazioni=
  + consideriamo allora il costo medio per i confronti: =costo ammortizzato=
    - costo ammortizzato nel futuro anche se costoso subito

- $2^k$ inserimenti, con M=1 inizialmente
  1. ogni inserimento tranne il primo ha costo $O(N)$
     - $T_{amm} = \frac{d + c + 2c + ... + (n-1)c}{n} \in O(n)$
       - sopra abbiamo una progressione aritmetica
         + numeratore di secondo grado, denominatore di primo grado
  2. $k$ inserimenti con $O(N)$, gli altri $O(1)$
     - $T_{amm}= \frac{(c+2c+...+2^{k-1}c)+2^kd}{2^k}$
     - $T_{amm}= \frac{(2^k -1)c+2^kd}{2^k} \in O(1)$
- sequenza di rimozioni di elementi
- sequenza di inserimenti, ma aumentando la dimensione dell'array di una costante se riempito

**** Extend
Si basa sull'espandere l'array quando esso diventa troppo piccolo
- l'espansione costa $O(N)$ in quanto richiede di allocare memoria e copiare gli elementi dell'array
#+begin-example
ArrayExtend(A,n):
  B = array[A.M + n]
  B.M = A.M + n
  B.N = A.N
  for i=1 to A.N do
    B[i] = A[i]
  return B
#+end-example

Il problema é che se N == M allora i successivi inserimenti richiedono ulteriori riallocazioni

**** Insert
#+begin-example
DynArrayInsert(A,k):
  if A.N == A.M then
    A = ArrayExtend(A,1)
  ArrayInsert(A,k)
#+end-example
Array non pieno: $O(1)$
Array pieno: $O(N)$
Dipende dalle operazioni precendenti
- se M é sufficientemente grande si sforeranno poche volte
  + il costo sará circa $O(1)$ ma si rischia di sprecare spazio
- se M é tale da sforare molte volte
  + il costo sará circa $O(N)$

Il problema é che se N == M allora i successivi inserimenti richiedono ulteriori riallocazioni
- raddoppiamo il numero di elementi se l'array si riempie
#+begin-example
DynArrayInsert(A,k):
  if A.N == A.M then
    A = ArrayExtend(A,A.M)
  ArrayInsert(A,k)
#+end-example

**** Delete
Possiamo recuperare spazio se l'array si riduce di dimensione
#+begin-example
DynArrayDelete2(A,k):
  ArrayDelete(A,k)
  if A.N <= 1/4 * A.M then
    B = array[A.M/2]
    B.M = A.M/2
    B.N = A.N
    for i=1 to A.N do
      B[i] = A[i]
    A = B
#+end-example

**** Search
** Liste
Una struttura lineare
- l'ordine é determinato dai puntatori che indicano l'elemento successivo
- data una lista L il primo elemento é indicano da L.head
Puó essere doppiamente concatenata
- con puntatori .prev e .next
Puó essere
- ordinata
- non ordinata
Puó essere circolare
- l'ultimo elemento punta il primo e viceversa
  + permette di accedere all'ultimo elemento piú facilmente

*** Insert
In Liste doppiamente concatenate e non ordinate:
#+begin-example
ListInsert(L,x):
  x.next = L.head
  x.prev = nil
  if L.head != nil then
    L.head.prev = x
  L.head = x
#+end-example
$O(1)$

Con sentinella:
#+begin-example
ListInsert(L,x):
  x.next = L.sen.next
  L.sen.next.prev = x
  L.sen.next = x
  x.prev = L.sen
#+end-example
$O(1)$

*** Delete
In Liste doppiamente concatenate e non ordinate:
- ricevendo un puntatore al nodo da rimuovere
#+begin-example
ListDelete(L,x):
  if x.prev != nil then
    x.prev.next = x.next
  else
    L.head = x.next
  if x.next != nil then
    x.next.prev = x.prev
#+end-example
$O(1)$

L'operazione é macchinosa perché bisogna controllare le condizioni "in testa" e "in coda"
- aggiungiamo nodo ~sentinella~ che rende piú omogenei i dati nella lista
  + Lista circolare
Si ha sempre la certezza che la lista contenga sempre almeno un elemento:
#+begin-example
ListDelete(L,x):
  x.prev.next = x.next
  x.next.prev = x.prev
#+end-example
$O(1)$ comunque tempo costante minore che senza sentinella
Ma il codice diventa piú semplice e leggibile

*** Search
In Liste doppiamente concatenate e non ordinate:
#+begin-example
ListSearch(L,x):
  x = L.head
  while x != nil and x.key != k do
    x = x.next
  return x
#+end-example
$O(N)$

Con sentinella:
#+begin-example
ListSearch(L,x):
  x = L.sen.next
  while x != L.sen and x.key != k do
    x = x.next
  return x
#+end-example
$O(N)$

** Hashing
*** Tavole a indirizzamente diretto
$U$ universo delle chiavi, tutte le chiavi possibili
- interi positivi tra 0 e m-1
L'insieme viene rappresentato con un array $T$ con dimensione $m$
- quindi si ha indirizzamento diretto

L'insieme delle chiavi $S$ é sottoinsieme di $U$
- ogni posizione di T contiene un puntatore hai dati con la chiave associata
#+begin-example
TableInsert(T,x):
  T[x.key] = x
TableDelete(T,x):
  T[x.key] = nil
TableSearch(T,k):
  return T[k]
#+end-example
Tutte le operazioni hanno complessita temporale $O(1)$

Sembra molto efficiente temporalmente
Non lo é sempre in termini di spazio
- se l'universo é molto grande occuperó molto spazio dovendo avere un puntatore per ognuna delle chiavi dell'universo

*** Tavole di hash
Possiamo utilizzare una tabella $T$ di dimensione $m$ molto piú piccola di $|U|$
- allora la posizione della chiave $k$ é determinata utilizzando una funzione di hash in quanto non c'é piú corrispondenza diretta tra chiave e indice
Quindi:
- l'indirizzamento non é piú diretto
- la posizione di $k$ é $h(k)$
- 2 chiavi possono corrispondere alla stessa posizione di $T$
  + una buona funzione hash riduce al minimo il numero di collisioni, cercando di distribuire in maniera casuale le coppie $k$ e posizione

*hash perfetto*
- funzione che non crea mai collisione, cioé una ~funzione iniettiva~:
  - $k_1 \neq k_2 \implies h(k_1) \neq h(k_2)$

*** Tavole hash con concatenamento
Cercano di risolvere le collisioni:
- usiamo liste per gestirle
  - allora se c'é collisione l'elemento é inserito in testa alla lista puntata dal puntatore nella posizione della array T

Il calcolo del hash ha tempo costante $O(1)$

#+begin-example
HashInsert(T,x):
  L = T[h(x.key)]
  ListInsert(L,x)
#+end-example
$O(1)$

#+begin-example
HashDelete(T,x):
  L = T[h(x.key)]
  ListDelete(L,x)
#+end-example
$O(1)$ perché la lista é doppiamente concatenata
- di un elemento giá individuato

#+begin-example
HashSearch(T,k):
  L = T[h(k)]
  return ListSearch(L,k)
#+end-example
La ricerca di un elemento richiede un tempo proporzionale alla lunghezza hella lista T[h(k)]
Definiamo:
- $m$ numero di celle in $T$
- $N$ numero di elementi memorizzati
- $\alpha = N / m$ fattore di carico
  - lunghezza media delle liste contenute nella tabella hash
~caso peggiore~:
- inserimenti con la stessa hash
- tutte le chiavi sono associate con la stessa cella di T
- ricerca $O(N)$
~caso migliore~:
- quando la lista T[h(k)] é vuota o contiene un solo elemento
- ricerca $O(1)$
~caso medio~:
- dipende dalla funzione hash
  - assumiamo $O(1)$
  - gode della proprietá di uniformitá semplice
    + =uniformitá semplice=
      + distribuisce in modo uniforme le chiavi fra le celle
        - ogni cella ha la stessa probabilitá di essere destinazione di una chiave casuale
- ricerca di un elemento non presente
  + individuare la lista é $\Theta(1)$
  + ogni lista ha la stessa probabilitá di essere associata con la chiave
  + percorrere la lista costa in media $\Theta(\alpha)$
  + infine il tempo richiesto é $\Theta(1+\alpha)$
    - $\alpha$ non é costante!
- ricerca di un elemento che c'é
  - dipende da dove l'elemento si colloca nella lista
  - il tempo di individuare la lista é $\Theta(1)$
  - assumiamo che la ricerca riguardi l'i-esimo elemento inserito
    + quanti di questi finiscono nella lista di $x_i$?
    + ogni elemento viene inserito nella lista di $x_i$ con probabilitá $\frac{1}{m}$
    + in media $\frac{N-i}{m}$ elementi precedono $x_i$ nella lista di $x_i$
  - il tempo di ricerca di $x_i$ é proporzionale a $1+\frac{N-i}{m}$
  - generalizzando alla ricerca di un elemento a caso
    + $\frac{1}{N} \sum_{i=1}^{N}(1+\frac{N-i}{m})$
    + $1+ \frac{\alpha}{2} - \frac{\alpha}{2N}$
  - in totale:
    + $\Theta(1) + \Theta(1+ \frac{\alpha}{2} - \frac{\alpha}{2N}) = \Theta(1+\alpha)$
Si conclude che se il numero di celle in T é proporzionale a N allora $N = O(m)$ e quindi $\alpha = O(1)$ e la ricerca richiede tempo $O(1)$

*** Funzioni hash
Solitamente la distribuzione secondo la quale si estagono le chiavi nno é nota
- non si puó creare una funzione hash perfetta
In un Computer le chiavi sono interpretate come sequenze di bit
- si cerca di utilizzare ogni bit della chiave
- una buona funzione hash sceglie posizioni in modo tale da eliminare eventuale regalaritá nei dati

**** Metodo della divisione
Veloce ma dipende da m
- m potenza di 2 é una buona scelta solo se si ha la certezza che gli ultimi bit hanno distribuzione uniforme

****   Metodo della moltiplicazione
m non é critico, solitamente si sceglie una potenza di 2
A dipende dai dati
- una scelta rogionevole empiricamente é $(\sqrt{5} - 1) / 2$
*** Indirizzamente aperto
Tutti gli elementi sono memorizzati nella tavola T
- l'elemento con chiave k viene inserito nella posizione h(k) se é libera
- se non é libera allora si cerca una posizione libera secondo uno ~schema di ispezione~
Possono avvenire collisioni anche tra elementi con chiavi diverse
In generale si definisce una funzione hash estese con l'ordene di ispezione
- $h: U \times \{0,1,2,...,m-1\} \to \{0,1,2,...,m-1\}$

=ispezione lineare=
- crea file di celle occupate, ovvero ~addensamento primario~
=ispezione quadratica=
- $h(k,i) = (h^{'}(k) + c_1i + c_2i^2) \mod m$
  - l'ordine con cui vengono esaminate le celle dipende solo dal hash della chiave k, ~addensamento secondario~
Per risolvere questo addensamento si introduce il =doppio hashing=
- $h(k,i) = (h_1(k) + ih_2(k)) \mod m$
  - permette una uguale probabilitá per ogni sequenza di ispezione


**** Insert
#+begin-example
HashInsert(T,x):
  for i=0 to i < m do
    j = h(x.key,i)
    if T[j] == nil then
      T[j] = x
      return j
  return nil
#+end-example

**** Search
#+begin-example
HashSearch(T,k):
  for i=0 to i < m do
    j = h(x.key,i)
    if T[j] == nil then
      return nil
    if T[j].key == k then
      return T[j]
  return nil
#+end-example

~caso ottimale~
- posizione di una chiave scelta a caso ha distribuzione uniforme
- qualunque sequenza di ispezione ha la stessa probabilitá
- Elemento Assente
  + $X$ numero di celle esaminate durante una ricerca senza successo
  + $X$ é una variabile aleatoria
    - almeno 1:
      - $P(X\ge 1)=1$
    - se la prima cella é occupata dovremo esaminare 2 celle:
      - $P(X\ge 2)= \frac{N}{m}$
    - se la seconda cella é occupata dovremo esaminare 3 celle:
      - $P(X\ge 3)= \frac{N}{m}\frac{N-1}{m-1}$
    - $P(X\ge i) \le \alpha^{i-1}}$
    - $E[X] = \sum_{i=1}^{\infty}(X \ge i) \le \sum_{i=1}^{\infty} \alpha^{i-1}=\frac{1}{1-\alpha}$
- Elemento Presente
  - sicuramente meno celle da esaminare che nel caso dell'elemento assente

**** Delete
Inserire nil creerebbe buchi nella tabella
- si potrebbe marcare le con costanti =deleted=
Di solito l'indirizzamento aperto si usa quando non c'é necessitá di cancellazione di elementi

** Pile
Inserisce chiavi in cima, rimuove le chiavi dalla cima
Come ADT:
- collezione di dati
  + elementi qualunque di tipo T
- operazioni
  + void Pusn(Stack S, T t)
  + T Pop(Stack S)
  + T Top(Stack S)
  + bool Empty(Stack S)
  + int Size(Stack S)

*** applicazioni
- chiamate ricorsive di funzioni
- visita in profonditá di grafi
- valutazione di un'espressione un notazione postfissa


*** assiomi
- Size(S), Empty(S), Push(S,t)
  + smepre definiti
- Pop(S), Top(S)
  + sono definite iff Empty(S) é false
- Empty(S)
  + true iff Size(S) é 0
- Push(S,t); Pop(S)
  + restituisce t e non modifica S
- Push(S,t); Top(S)
  + restituisce t
- Push(S,t)
  + incremento Size(S) di uno
- Pop(S)
  + decrementa Size(S) di uno

*** implementazione con array
statico di M celle
- assioma ulteriore
  + Push(S,t)
    - iff Size(S) < M
meccanismo =LIFO=

Complessitá
- Temporale $O(1)$
- Spaziale $O(M)$


#+begin_example
Push(S,t)
  if S.N != S.M then
    S.N = S.N + 1
    S[N] = t
  else
    error overflow
#+end_example

#+begin_example
Size(S)
  return S.N
#+end_example

#+begin_example
Empty(S)
  if S.N == 0 then
    return true
  else
    return false
#+end_example

#+begin_example
Top(S)
  if S.N == 0 then
    error underflow
  else
    return S[S.N]
#+end_example

#+begin_example
Pop(S)
  if S.N == 0 then
    error underflow
  else
    S.N = S.N - 1
    return S[S.N+1]
#+end_example

*** implementazione con lista
Conviene una lista semplice con sentinella
Complessitá
- Temporale $O(1)$
- Spaziale $O(N)$
  + ma con overhead dovuto ai puntatori
  + non abbiamo un limite al massimo degli elementi
#+begin_example
Push(S,t)
  S.N = S.N + 1
  t.next = S.sen.next
  S.sen.next = t
#+end_example

#+begin_example
Size(S)
  return S.N
#+end_example
#+begin_example
Empty(S)
  if S.N == 0 then
    return true
  else
    return false
#+end_example
#+begin_example
Top(S)
  if S.N == 0 then
    error underflow
  else
    return S.sen.next
#+end_example

#+begin_example
Pop(S)
  if S.N == 0 then
    error underflow
  else
    S.N = S.N - 1
    t = S.sen.next
    S.sen.next = S.sen.next.next
    return t
#+end_example


** Code
- collezione di dati
  + elementi qualunque di tipo T
- operazioni
  + void Enqueue(Queue Q, T t)
  + T Dequeue (Queue Q)
  + T Front(Queue Q)

- Utilizzi
  + buffer
  + visita in ampiezza di grafi
  + simulazione di eventi complessi

*** assiomi
- Size Empty Enqueue
  + sempre definiti
- dequeue front
  + definiti iff Empty é false
- empty size e front
  + non modificano la coda
- empty
  + true iff Size é 0
- size; enqueue(Q,t)
  + N, dopo N esecuzioni di Dequeue(Q) abbiamo Front(Q) = t
- Front(Q) = t ==> Dequeue(Q) = t
- Enqueue(Q,t) incrementa Size(Q) di 1
- Dequeue(Q) decrementa Size(Q) di 1

*** implementazione con array
statico
si temgono elementi nelle ultime posizioni
- altrimenti sarebbe necessario spostare tutti gli elementi
si usa l'array un maniera circolare
- si utilizzano riferimenti tail e head per tener conto delle posizioni dell'array
  + Q.head indica dove estrarre
  + Q.tail indica dove inserire
    - se head == tail allora la coda é vuota

Complessitá
- Temporale $O(1)$
- Spaziale $O(M)$
#+begin_example
Size(Q)
  if Q.tail >= Q.head then
    return Q.tail - Q.head
  else
    return Q.M - (Q.head - Q.tail)
#+end_example

#+begin_example
Empty(Q)
  if Q.tail == Q.head then
    return true
  else
    return false
#+end_example
#+begin_example
NextCell(Q,c)
  if c != Q.M then
    return c+1
  else
    return 1
#+end_example
#+begin_example
Enqueue(Q,t)
  if Q.tail == Q.head-1 then
    error overflow
  else
    Q[Q.tail] = t
    Q.tail = NextCell(Q,Q.tail)
#+end_example
#+begin_example
Front(Q)
  if Q.tail == Q.head then
    error underflow
  else
    return Q[Q.head]
#+end_example
#+begin_example
Dequeue(Q)
  if Size(Q) == 0 then
    error underflow
  else
    t = Q[Q.head]
    Q.head = NextCell(Q,Q.head)
    return t
#+end_example
*** implementazione con lista
inserimenti in testa, estrazioni in coda
- lista semplice con puntatore all'ultimo elemento della coda

Q.head punta l'elemento da estrarre
Q.tail punta l'ultimo elemento inserito
Q.head == nil <==> la coda é vuota
Q.N tiene conto del numero di elementi


Complessitá
- Temporale $O(1)$
- Spaziale $O(N)$
  - con overhead dei puntatori
  - non abbiamo il vincolo del massimo degli elementi

#+begin_example
Enqueue(Q,t)
  if Q.N == 0 then
    Q.head = t
    Q.tail = t
  else
    Q.tail.next = t
    Q.tail = t
  Q.N = Q.N +1
#+end_example
#+begin_example
Size(Q)
  return Q.N
#+end_example
#+begin_example
Empty(Q)
  return Q.N == 0
#+end_example
#+begin_example
Front(Q)
  if Empty(Q) then
    error underflow
  else
   return Q.head
#+end_example

#+begin_example
Dequeue(Q)
  if Empty(Q) then
    error underflow
  else
    t = Q.head
    Q.head = Q.head.next
    Q.N = Q.N -1
    return t
#+end_example

** Alberi
Strutture gerarchiche:
$a \in A \land T_1 \in T(A) \land T_2 \in T(A) \land ... \land T_k \in T(A)\implies \{a,T_1,T_2,...,T_k\} \in T(A)$
- k >= 0
- A insieme di etichette
- T(A) insieme di alberi su A
- a radice

- Un =albero= é un grafo connesso aciclico
  + una =radice= é un nodo privilegiato
  + una =foglia= é un nodo da cui non esce alcun arco
  + se un nodo non é foglia é interno
  + il grado di un albero é il massimo numero di figli di un nodo
  + un insieme di alberi é una =foresta=
- Ma un grafo non é detto sia un albero

=Cammino=
- sequenza di archi cascuno incidente sul vertice di quello successivo
- un cammino da una radice ad una foglia si dice =ramo=
=Livelli=
- insiemi di vertici equidistandi dalla radice
- =altezza=
  + massima distanza dalla radice di un livello non vuoto (n. livelli -1)

Le operazioni di Cardinalitá e Altezza
- hanno complessitá temporale simile a quella di visita
  - $O(n)$

*** Binary Trees
=Alberi binari posizionali=
$BT(A)$ definito induttivamente
- albero vuoto
- sottoalberi sinistro destro

Rappresentato all'interno di un Computer:
- key (etichetta)
  + left (puntatore)
  + right (puntatore)

#+begin_example
BTCard(Tree T)
  if(T == nil)
    return 0
  else
    return 1 + BTCard(T.left) + BTCard(T.right)
  end if
#+end_example
#+begin_example
BTHeight(Tree T)
  if(T == nil)
    return -1
  else
    l = BTHeight(T.left)
    r = BTHeight(T.right)
    return 1 + max(l,r)
  end if

#+end_example
La complessitá sará uguale a quella di una visita (completa) di un albero


*** K-ary Trees
per ogni nodo
- key
- lista di puntatori

É possibile codificare ogni albero k-ario con un albero binario
- ogni nodo punta al primo figlio e al primo fratello
- key
  + child
  + sibling
- si perde la connessione diretta tra genitori e figli non primi

#+begin_example
k-TCard(Tree T)
  if(T == nil)
    return 0
  else
    card = 1
    C = T.child
    while C != nil do
      card = card + k-TCard(C)
      C = C.sibling
    end while
    return card
  end if
#+end_example
Ma la rappresentazione binaria permetterebbe anche l'algoritmo BTCard()
#+begin_example
k-THeight(Tree T)
  if(T.child == nil)
    return 0
  else
    h = 0
    C = T.child
    while C != nil do
      h = max(h,k_THeight(C))
      C = C.sibling
    return h+1
  end if
#+end_example
In questo caso non é possbile utilizzare l'algoritmo per alberi binari

*** Visite
Stessa complessitá degli algoritmi di Cardinalitá e Altezza per alberi

**** In profonditá DFS
DFS con preordine sinistro
#+begin_example
Tree-DFS(k-Tree T)
  visit T.key
  C = T.child
  while C != nil do
    Tree-DFS(C)
    C = S.sibling
  end while
#+end_example

DFS iterativo con preordine destro utilizzando uno Stack
- l'ordine di visita é l'ordine di rimozione dallo Stack
#+begin_example
Tree-DFS-Stack(k-Tree T)
  S = empty stack
  Push(S,T)
  while S != empty stack do
    T' = Pop(S)
    visits T'.key
    for all C child of T' do
      Push(S,C)
    end for
  end while
#+end_example
**** In ampiezza BFS
Non operabile con una ricorsione
Iterativo utilizzando una coda
- l'ordine di visita é l'ordine di rimozione dalla Coda
#+begin_example
Tree-BFS(k-Tree T)
  Q = empty queue
  Enqueue(Q,T)
  while Q != empty queue do
    T' = Dequeue(Q)
    visits T'.key
    for all C child of T' do
      Enqueue(Q,C)
    end for
  end while
#+end_example

**** Complessitá
- in base alla cardinalitá n dell'albero
- il numero di cicli dipendono molto dalla struttura dell'albero
  + possiamo contare il numero di operazioni Push/Pop e Enqueue/Dequeue
  + ogni nodo dell'albero viene inserito ed estratto esattamente una volta
    - $O(2n) = O(n)$
      - sono ottimi in quanto per visitare un albero bisogna visitare almeno $n$ volte l'albero

*** Alberi di Ricerca
Definizione induttiva
- non basta fare verifiche localmente
Con una radice $a$ con agganciati due BRT $l$ e $r$
- tutti gli elementi di $l$ sono minori di $a$
- tutti gli elementi di $r$ sono maggiori di $a$
Questa regola vale per tutto l'albero

$n$ numero dei nodi
$h$ numero di archi che compongono il ramo piú lungo

i BRT possono diventare anche molto sbilanciati
- dipende dall'ordine in cui vengono inseriti gli elementi
#+begin_example
Ric-Search(x, T)
  if x < T.key then
    Search(x,T.left)
  else if x > T.key then
    Search(x,T.right)
  else
    return T
  end if
#+end_example
Complessitá $O(h)$ con $h$ altezza di $T$ nel caso peggiore

#+begin_example
It-Search(x, T)
  Tree S = T
  while S != nil and S.key != x do
    if S.key < x then
      S = S.right
    else
      S = S.left
    end if
    return S
#+end_example

Stampa in Ordine
- pre: T binario di ricerca
- post: stampate le chiavi di T in ordine
#+begin_example
Print-Inorder(T)
  if T != nil
    Print-Inorder(T.left)
    print(T.key)
    Print-Inorder(T.right)
  end if
#+end_example

Minimo
#+begin_example
Tree-Min(T)
  if T.left != nil then
    return Tree-Min(T.left)
  end if
  return T
#+end_example

Massimo
#+begin_example
Tree-Max(T)
  if T.right != nil then
    return Tree-Max(T.right)
  end if
  return T
#+end_example

Successore
- pre: N nodo di un albere bin. di ricerca
- post: il successore di N se esiste, /nil/ altrimenti
#+begin_example
Tree-Successor(N)
  if N.right == nil then
    Tree P = N
    while P.right == N and P != nil do
      N = P
      P = N.parent
    end while
    return P
  else
    return Tree-Min(N.right)
  end if
#+end_example

Un inserimento avviene a livello delle foglie sostituendo un sottoalbero vuoto (/nil/) in modo che l'albero rimanga albero di ricerca

#+begin_example
Tree-Insert(N,T)
  P = nil
  S = T
  while S != nil
    P = S
    if N.key = S.key then
      return
    else
      if N.key < S.key then
        S = S.left
      else
        S = S.right
      end if
    end if
  end while
  N.parent = P
  if P == nil then
    T = N
  else
    if N.key < P.key then
      P.left = N
    else
      P.right = N
    end if
  end if
#+end_example

Cancellazione
Nel caso particolare che N abbia un solo figlio

#+begin_example
1-Delete(N,T)
  if N == T then
    if N.left != nil and N.right == nil then
      T = N.left
    else
      T = N.right
    end if
    N.parent = nil
  else
    P = N.parent
    if N == P.left then
      if N.left != nil and N.right == nil then
        P.left = N.left
        N.left.parent = P
      else
        P.left = N.right
        N.right.parent = P
      end if
    else
      if N.left != nil and N.right == nil then
        P.right = N.left
        N.left.parent = P
      else
        P.right = N.right
        N.right.parent = P
      end if
    end if
  end if
#+end_example

Quindi risolvendo il caso piú generale:

#+begin_example
Tree-Delete(Z,T)
  if Z.left == nil and Z.right == nil then
    if Z == T then
      T = nil
    else
      if Z.parent.left = Z then
        Z.parent.left = nil
      else
        Z.parent.right = nil
  else
    if Z.left == nil or Z.right == nil then
      1-Delete(Z,T)
    else
      Y = Tree-Min(Z.right)
      Z.key = Y.key
      Tree-Delete(Y,T)
#+end_example

Salvataggio in lista
- inserire gli elementi di un BRT in ordine in una lista
- stessa tecnica per Print-Tree
- O(n) best case sbilanciato a destra
- O(n^2) worst case sbilanciato a sinistra
#+begin_example
ToList-InOrder(T)
  if T == nil then
    return nil
  else
    L = ToList-InOrder(T.left)
    R = ToList-InOrder(T.right)
    R = ListInsert(T.key,R)
    return Append(L,R)
#+end_example

Visitando i nodo in ordine decrescente di etichette si evita l'utilizza di Append
- O(n)
#+begin_example
ToList-InOrder(T,L)
if T == nil then
  return L
else
  L = ToList-InOrder(T.right,L)
  L = ListInsert(T.key,L)
  return ToList-InOrder(T.left,L)
#+end_example

Copia isomorfo di un albero
- inserimenti successivi di una lista dei nodi visitati in preordine di T

**** Red-Black
=Alberi binari di ricerca bilanciati=
Questo é utile in alberi in cui le operazioni in cui l'altezza conta sono usate spesso
- in un albero sbilanciato
  - $h = n-1$
- un albero bilanciato
  - $h$ proporzionale al logaritmo di $n$
  - $n=2^h$
Senza meccanismi particolari la forma dell'albero dipende solamente dall'ordine dell'inserimento

$R-N$ é un BRT aumentato, i cui vertici sono colorati di rosso o nero:
- nero: la radice e tutte le foglie (/nil/) sono nere
- rosso: se un nodo é rosso tutti i suoi figli sono neri
  + nodi adiacenti non possono essere entrambi rossi
- cammino: per ogni nodo x tutti i cammini da x ad una foglia hanno lo stesso numero di nodi neri

$bh(x) =$ altezza nera di x
- numero di nodi neri su un ramo a x ad una foglia(/nil/) (x escluso)

=Proposizione=
- l'altezza massima di un albero R-N con n nodi é $2 \log_2(n+1)$
  - limite massimo di $h$

Ricerca $O(\log n)$
Inserimento $O(\log n)$
Cancellazione $O(\log n)$

Queste peró devono mantenere l'albero bilanciato
=Rotazione=
- dopo una rotazione l'albero rimane di ricerca
- se non rispettava le regole R-N dopo le rispetta
#+begin_example
Left-Rotate(T,x)
  y = x.right
  x.right = y.left
  y.parent = x.parent
  if x.parent == nil then
    T = y
  else
    if x.parent.left == x then
      x.parent.left = y
    else
      x.parent.right = y
  if x.right != nil then
    x.right.parent = x
  y.left = x
  x.parent = y
#+end_example
